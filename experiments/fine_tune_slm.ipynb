{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "734d4e33-495d-48dc-9e4e-c05b47c9cabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding,\n",
    "    TrainingArguments, Trainer\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import evaluate\n",
    "\n",
    "# Load dataset\n",
    "def load_and_preprocess_data(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df = df.rename(columns={'instruction': 'text'})\n",
    "    \n",
    "    # Sample 10% per category\n",
    "    df = df.groupby(\"category\").apply(lambda x: x.sample(frac=0.1, random_state=42)).reset_index(drop=True)\n",
    "    \n",
    "    # Filter relevant categories\n",
    "    categories = ['CARD', 'LOAN', 'TRANSFER', 'FEES', 'ACCOUNT', 'CONTACT', 'ATM']\n",
    "    df = df[df['category'].isin(categories)][['text', 'category']].reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Encode labels\n",
    "def encode_labels(df):\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['label'] = label_encoder.fit_transform(df['category'])\n",
    "    return df, label_encoder\n",
    "\n",
    "# Split dataset\n",
    "def split_dataset(df):\n",
    "    xtrain, xtest = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
    "    return xtrain.reset_index(drop=True), xtest.reset_index(drop=True)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "def convert_to_hf_dataset(xtrain, xtest):\n",
    "    return DatasetDict({\n",
    "        \"train\": Dataset.from_pandas(xtrain),\n",
    "        \"validation\": Dataset.from_pandas(xtest)\n",
    "    })\n",
    "\n",
    "# Tokenization\n",
    "def tokenize_function(tokenizer, examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], truncation=True, max_length=512, return_tensors=\"np\"\n",
    "    )\n",
    "\n",
    "# Load model and tokenizer\n",
    "def load_model_and_tokenizer(model_checkpoint, num_labels, id2label, label2id):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_checkpoint, num_labels=num_labels, id2label=id2label, label2id=label2id\n",
    "    )\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# Compute accuracy\n",
    "def compute_metrics(p):\n",
    "    accuracy = evaluate.load(\"accuracy\")\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": accuracy.compute(predictions=predictions, references=labels)}\n",
    "\n",
    "# Fine-tune model using PEFT\n",
    "def fine_tune_with_lora(model):\n",
    "    peft_config = LoraConfig(task_type=\"SEQ_CLS\", r=4, lora_alpha=32, lora_dropout=0.01, target_modules=['q_lin'])\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    return model\n",
    "\n",
    "# Training setup\n",
    "def train_model(model, tokenizer, tokenized_dataset):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"distilbert-lora-text-classification\",\n",
    "        learning_rate=1e-3,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        num_train_epochs=5,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    return model\n",
    "\n",
    "# Make predictions\n",
    "def predict(model, tokenizer, id2label, text_list, device=\"mps\"):\n",
    "    model.to(device)\n",
    "    print(\"Trained Model Predictions:\")\n",
    "    print(\"--------------------------\")\n",
    "    \n",
    "    for text in text_list:\n",
    "        inputs = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "        logits = model(inputs).logits\n",
    "        predictions = torch.argmax(logits).item()\n",
    "        print(f\"{text} - {id2label[predictions]}\")\n",
    "\n",
    "# Save Model\n",
    "def save_model(model, save_path):\n",
    "    model.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48923d4e-80bb-4306-b222-ed021fe4207c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2c/dhkr62756zs8gc_73ctttk840000gn/T/ipykernel_7679/3110375347.py:21: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby(\"category\").apply(lambda x: x.sample(frac=0.1, random_state=42)).reset_index(drop=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'category', 'label'],\n",
       "        num_rows: 1747\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'category', 'label'],\n",
       "        num_rows: 437\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FILE_PATH = \"hf://datasets/bitext/Bitext-retail-banking-llm-chatbot-training-dataset/bitext-retail-banking-llm-chatbot-training-dataset.parquet\"\n",
    "MODEL_CHECKPOINT = 'distilbert-base-uncased'\n",
    "\n",
    "df = load_and_preprocess_data(FILE_PATH)\n",
    "df, label_encoder = encode_labels(df)\n",
    "xtrain, xtest = split_dataset(df)\n",
    "dataset = convert_to_hf_dataset(xtrain, xtest)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee2f217f-90f8-4f84-8754-51ba97665012",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2728a9aab3749b3aac679a2a0db78a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1747 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b3b232140749e1a1c6b69dcc4205be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/437 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/2c/dhkr62756zs8gc_73ctttk840000gn/T/ipykernel_7679/3110375347.py:94: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 632,839 || all params: 67,591,694 || trainable%: 0.9363\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2185' max='2185' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2185/2185 01:28, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.054786</td>\n",
       "      <td>{'accuracy': 0.9862700228832952}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.236300</td>\n",
       "      <td>0.034092</td>\n",
       "      <td>{'accuracy': 0.988558352402746}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.032200</td>\n",
       "      <td>0.041158</td>\n",
       "      <td>{'accuracy': 0.9862700228832952}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.028028</td>\n",
       "      <td>{'accuracy': 0.9862700228832952}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.025204</td>\n",
       "      <td>{'accuracy': 0.9862700228832952}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): DistilBertForSequenceClassification(\n",
       "      (distilbert): DistilBertModel(\n",
       "        (embeddings): Embeddings(\n",
       "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "          (position_embeddings): Embedding(512, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (transformer): Transformer(\n",
       "          (layer): ModuleList(\n",
       "            (0-5): 6 x TransformerBlock(\n",
       "              (attention): DistilBertSdpaAttention(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (q_lin): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.01, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (ffn): FFN(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (activation): GELUActivation()\n",
       "              )\n",
       "              (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pre_classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=7, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=7, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label = {0: \"ACCOUNT\", 1: \"ATM\", 2:\"CARD\", 3:\"CONTACT\", 4:\"FEES\", 5:\"LOAN\", 6:\"TRANSFER\"}\n",
    "label2id = {\"ACCOUNT\":0, \"ATM\":1, \"CARD\":2, \"CONTACT\":3, \"FEES\":4, \"LOAN\":5, \"TRANSFER\":6}\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(MODEL_CHECKPOINT, num_labels=len(id2label), id2label=id2label, label2id=label2id)\n",
    "tokenized_dataset = dataset.map(lambda x: tokenize_function(tokenizer, x), batched=True)\n",
    "\n",
    "model = fine_tune_with_lora(model)\n",
    "model = train_model(model, tokenizer, tokenized_dataset)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94bbcdf1-2229-4abf-b832-85f9c6a80ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Model Predictions:\n",
      "--------------------------\n",
      "What is the eligibility criteria for a home loan - LOAN\n",
      "How to apply for a credit card - CARD\n",
      "What is the process of getting a loan approved. - LOAN\n",
      "Can I get a loan - LOAN\n",
      "I want to transfer money - TRANSFER\n"
     ]
    }
   ],
   "source": [
    "text_samples = [\n",
    "    \"What is the eligibility criteria for a home loan\", \n",
    "    \"How to apply for a credit card\", \n",
    "    \"What is the process of getting a loan approved.\", \n",
    "    \"Can I get a loan\", \n",
    "    \"I want to transfer money\"\n",
    "]\n",
    "\n",
    "predict(model, tokenizer, id2label, text_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d2b8e06-f2ea-4151-bf1c-41972e25eee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, \"bank_fineuned_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac466b7-1d18-4f39-8e8d-8b7fa6b27cab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ac0947-5f3f-483b-a479-95e7fa157a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
